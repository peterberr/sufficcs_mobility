{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to make summary stats of travel in combined DE other and FR other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from math import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickled dictionary files\n",
    "with open('../dictionaries/Germany_var.pkl','rb') as f:\n",
    "    var_all = pickle.load(f)\n",
    "\n",
    "with open('../dictionaries/Germany_val.pkl','rb') as f:\n",
    "    value_all = pickle.load(f)\n",
    "\n",
    "with open('../dictionaries/Germany_na.pkl','rb') as f:\n",
    "    na_all = pickle.load(f)\n",
    "\n",
    "with open('../dictionaries/city_postcode_DE.pkl','rb') as f:\n",
    "    city_plz = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sP_sH_sW(city):\n",
    "    fn_hh='../../MSCA_data/SrV/' + city + '/SrV2018_Einzeldaten_' + city + '_SciUse_H2018.csv'\n",
    "    fn_p='../../MSCA_data/SrV/' + city + '/SrV2018_Einzeldaten_' + city + '_SciUse_P2018.csv'\n",
    "    fn_t='../../MSCA_data/SrV/' + city + '/SrV2018_Einzeldaten_' + city + '_SciUse_W2018.csv'\n",
    "\n",
    "    sH=pd.read_csv(fn_hh,encoding='latin_1',sep=';',dtype={'PLZ':str,'GEWICHT_HH':str})\n",
    "    sH.dropna(subset=['HHNR'],inplace=True)\n",
    "    sP=pd.read_csv(fn_p,encoding='latin_1',sep=';')\n",
    "    sW=pd.read_csv(fn_t,encoding='latin_1',sep=';',dtype={'V_START_PLZ':str,'V_ZIEL_PLZ':str})\n",
    "\n",
    "    for k in var_all['HH'].keys():\n",
    "        if len(value_all['HH'][k])>0:\n",
    "            sH[k]=sH[var_all['HH'][k]].map(value_all['HH'][k])\n",
    "        elif len(value_all['HH'][k])==0:\n",
    "            sH[k]=sH[var_all['HH'][k]]\n",
    "\n",
    "    for k in var_all['P'].keys():\n",
    "        if len(value_all['P'][k])>0:\n",
    "            sP[k]=sP[var_all['P'][k]].map(value_all['P'][k])\n",
    "        elif len(value_all['P'][k])==0:\n",
    "            sP[k]=sP[var_all['P'][k]]\n",
    "\n",
    "    for k in var_all['W'].keys():\n",
    "        if len(value_all['W'][k])>0:\n",
    "            sW[k]=sW[var_all['W'][k]].map(value_all['W'][k])\n",
    "        elif len(value_all['W'][k])==0:\n",
    "            sW[k]=sW[var_all['W'][k]]\n",
    "\n",
    "    # fill in na's as necessary\n",
    "    sH.fillna(value=na_all['HH'],inplace=True)\n",
    "    sP.fillna(value=na_all['P'],inplace=True)\n",
    "    sW.fillna(value=na_all['W'],inplace=True)\n",
    "\n",
    "    # keep only variables needed, i.e. the variables that are included as dictionary keys\n",
    "    sH=sH[list(value_all['HH'].keys())]\n",
    "    sP=sP[list(value_all['P'].keys())]\n",
    "    sW=sW[list(value_all['W'].keys())]\n",
    "\n",
    "    # change decimal from , to . and convert those variables from string to float\n",
    "    sH.loc[:,'HH_Weight']=sH.loc[:,'HH_Weight'].map(lambda x: x.replace(',','.')).astype('float')\n",
    "    sP.loc[:,'Per_Weight']=sP.loc[:,'Per_Weight'].map(lambda x: x.replace(',','.')).astype('float')\n",
    "    sW.loc[:,'Trip_Weight']=sW.loc[:,'Trip_Weight'].map(lambda x: x.replace(',','.')).astype('float')\n",
    "    sW.loc[:,'Trip_Distance']=sW.loc[:,'Trip_Distance'].map(lambda x: x.replace(',','.')).astype('float')\n",
    "    sW.loc[:,'Trip_Distance_GIS']=sW.loc[:,'Trip_Distance_GIS'].map(lambda x: x.replace(',','.')).astype('float')\n",
    "\n",
    "    # define unique person and trip numbers\n",
    "    sP['HH_PNR']=sP['HHNR'].astype('str')+'_'+sP['Person'].astype('str')\n",
    "    sW['HH_PNR']=sW['HHNR'].astype('str')+'_'+sW['Person'].astype('str')\n",
    "    sW['HH_P_WNR']=sW['HH_PNR']+'_'+sW['Trip'].astype('str')\n",
    "\n",
    "    # bring HHNR and geo_unit to the left side of the HH df\n",
    "    cols=sH.columns.tolist()\n",
    "    cols_new = ['HHNR', 'Res_geocode'] + [value for value in cols if value not in {'HHNR', 'Res_geocode'}]\n",
    "    sH=sH[cols_new]\n",
    "\n",
    "    # bring HHNR, HH_PNR, to the left side of the Per df\n",
    "    cols=sP.columns.tolist()\n",
    "    cols_new = ['HHNR','HH_PNR'] + [value for value in cols if value not in {'HHNR', 'HH_PNR'}]\n",
    "    sP=sP[cols_new]\n",
    "\n",
    "    # bring HHNR, HH_PNR, HH_P_WNR to the left side of the W df\n",
    "    cols=sW.columns.tolist()\n",
    "    cols_new = ['HHNR','HH_PNR','HH_P_WNR'] + [value for value in cols if value not in {'HHNR', 'HH_PNR','HH_P_WNR'}]\n",
    "    sW=sW[cols_new]\n",
    "\n",
    "    # count company cars towards car ownership\n",
    "    sH.loc[sH['CompanyCarHH']==1,'CarOwnershipHH']=1\n",
    "\n",
    "    sW['Hour']=sW.loc[:,'Time']\n",
    "\n",
    "    # Define trip time categories\n",
    "    sW['Trip_Time']='Nighttime Off-Peak'\n",
    "    sW.loc[sW['Hour'].isin([6,7,8,9]),'Trip_Time']='AM_Rush'\n",
    "    sW.loc[sW['Hour'].isin([12,13]),'Trip_Time']='Lunch'\n",
    "    sW.loc[sW['Hour'].isin([16,17,18]),'Trip_Time']='PM Rush'\n",
    "    sW.loc[sW['Hour'].isin([19,20,21]),'Trip_Time']='Evening'\n",
    "    sW.loc[sW['Hour'].isin([10,11,14,15]),'Trip_Time']='Daytime Off-Peak'\n",
    "\n",
    "    # remove rows with NA age\n",
    "    sP=sP.loc[sP['Age']>=0,]\n",
    "\n",
    "    sW.drop(columns=['HHNR','Person', 'Ori_Reason', 'Des_Reason','Time'],inplace=True)\n",
    "    sW=sW.loc[sW['Trip_Valid']==1,:]\n",
    "    # merge together the household, person, and trip files\n",
    "    sHP=sH.merge(sP,on='HHNR')\n",
    "    sHPW=sHP.merge(sW,on='HH_PNR')\n",
    "\n",
    "    cols=sHPW.columns.tolist()\n",
    "    cols_new = ['HHNR','HH_PNR','HH_P_WNR','Res_geocode','Ori_Plz','Des_Plz','Trip_Time','Trip_Purpose'] + [value for value in cols if value not in {'HHNR','HH_PNR','HH_P_WNR','Res_geocode','Ori_Plz','Des_Plz','Trip_Time','Trip_Purpose','Mode', 'Trip_Distance'}] +['Mode', 'Trip_Distance']\n",
    "    sHPW=sHPW[cols_new]\n",
    "\n",
    "    # responses should be of people who live in the city, and trips should either start or end in the city\n",
    "    # same for other dataframes\n",
    "    sP=sP.merge(sH.loc[:,['HHNR','Res_geocode']])\n",
    "    sW=sW.merge(sP.loc[:,['HH_PNR','Res_geocode']])\n",
    "\n",
    "    sH=sH.loc[sH['Res_geocode'].isin(city_plz[city]),:]\n",
    "    sP=sP.loc[sP['Res_geocode'].isin(city_plz[city]),:]\n",
    "    sHP=sHP.loc[sHP['Res_geocode'].isin(city_plz[city]),:]\n",
    "    sW=sW.loc[sW['Res_geocode'].isin(city_plz[city]),:]\n",
    "    sW=sW.loc[(sW['Ori_Plz'].isin(city_plz[city])) | (sW['Des_Plz'].isin(city_plz[city]))]\n",
    "\n",
    "    sP['City']=city\n",
    "    sH['City']=city\n",
    "    sW['City']=city\n",
    "\n",
    "    return sP, sH, sW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_3032\\2649625857.py:9: DtypeWarning: Columns (35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sW=pd.read_csv(fn_t,encoding='latin_1',sep=';',dtype={'V_START_PLZ':str,'V_ZIEL_PLZ':str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leipzig added.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_3032\\2649625857.py:9: DtypeWarning: Columns (35,37,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sW=pd.read_csv(fn_t,encoding='latin_1',sep=';',dtype={'V_START_PLZ':str,'V_ZIEL_PLZ':str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leipzig added again.\n",
      "Magdeburg added.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_3032\\2649625857.py:9: DtypeWarning: Columns (35,37,77) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sW=pd.read_csv(fn_t,encoding='latin_1',sep=';',dtype={'V_START_PLZ':str,'V_ZIEL_PLZ':str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magdeburg added again.\n",
      "Potsdam added.\n",
      "Potsdam added again.\n",
      "Frankfurt am Main added.\n",
      "Frankfurt am Main added again.\n",
      "Düsseldorf added.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_3032\\2649625857.py:8: DtypeWarning: Columns (28,41,73,76,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sP=pd.read_csv(fn_p,encoding='latin_1',sep=';')\n",
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_3032\\2649625857.py:9: DtypeWarning: Columns (35,37,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sW=pd.read_csv(fn_t,encoding='latin_1',sep=';',dtype={'V_START_PLZ':str,'V_ZIEL_PLZ':str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Düsseldorf added again.\n",
      "Kassel added.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_3032\\3614051921.py:15: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1=pd.read_csv('../outputs/Combined/' + city1 + '_UF.csv',dtype={'Res_geocode':str})\n",
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_3032\\2649625857.py:9: DtypeWarning: Columns (35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sW=pd.read_csv(fn_t,encoding='latin_1',sep=';',dtype={'V_START_PLZ':str,'V_ZIEL_PLZ':str})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kassel added again.\n"
     ]
    }
   ],
   "source": [
    "city0='Dresden'\n",
    "df0=pd.read_csv('../outputs/Combined/' + city0 + '_UF.csv',dtype={'Res_geocode':str})\n",
    "df0['Commute_Trip']=0\n",
    "df0.loc[df0['Trip_Purpose_Agg']=='Home↔Work','Commute_Trip']=1\n",
    "df0['City']=city0\n",
    "df_all=df0.copy()\n",
    "\n",
    "sP0, sH0, sW0=get_sP_sH_sW(city0)\n",
    "sP_all=sP0.copy()\n",
    "sH_all=sH0.copy()\n",
    "sW_all=sW0.copy()\n",
    "cities0=['Leipzig','Magdeburg','Potsdam','Frankfurt am Main','Düsseldorf','Kassel']\n",
    "for city1 in cities0:\n",
    "        #print(city1)\n",
    "        df1=pd.read_csv('../outputs/Combined/' + city1 + '_UF.csv',dtype={'Res_geocode':str})\n",
    "        df1['Commute_Trip']=0\n",
    "        df1.loc[df1['Trip_Purpose_Agg']=='Home↔Work','Commute_Trip']=1\n",
    "        df1['City']=city0\n",
    "        if len(df1.columns==df_all.columns):\n",
    "                df_all=pd.concat([df_all,df1])\n",
    "                print(city1, 'added.')\n",
    "\n",
    "        sP1, sH1, sW1=get_sP_sH_sW(city1)\n",
    "        if len(sP1.columns)==len(sP_all.columns):# & (sH1.columns==sH_all.columns) & (sW1.columns==sW_all.columns):\n",
    "                sP_all=pd.concat([sP_all,sP1])\n",
    "                sH_all=pd.concat([sH_all,sH1])\n",
    "                sW_all=pd.concat([sW_all,sW1])\n",
    "                print(city1, 'added again.')\n",
    "\n",
    "\n",
    "\n",
    "df_DE=df_all.copy()\n",
    "sP_DE=sP_all.copy()\n",
    "sH_DE=sH_all.copy()\n",
    "sW_DE=sW_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sHPW=df_DE.copy()\n",
    "sP=sP_DE.copy()\n",
    "sH=sH_DE.copy()\n",
    "sW=sW_DE.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person weights and trip weights are all the same:  True\n"
     ]
    }
   ],
   "source": [
    "# daily distance per person\n",
    "daily_dist=sHPW.groupby('HH_PNR')['Trip_Distance'].sum().to_frame().reset_index()\n",
    "# daily distance per person, including people with out travel distances, for a variety of reasons\n",
    "daily_dist_all=daily_dist.merge(sP['HH_PNR'],how='right')\n",
    "# get the household-person ids for those who did travel, but their travels were recorded as invalid for some reason\n",
    "HH_PNR_na=sW.loc[sW['HH_PNR'].isin(daily_dist_all.loc[daily_dist_all['Trip_Distance'].isna(),'HH_PNR']),'HH_PNR']\n",
    "na_PNR=HH_PNR_na.drop_duplicates().values.tolist()\n",
    "\n",
    "\n",
    "weighted=sHPW.loc[:,('HH_PNR','Per_Weight','Mode','Trip_Distance','Trip_Purpose_Agg')]\n",
    "weighted=weighted.loc[~weighted['HH_PNR'].isin(na_PNR),:]\n",
    "weighted['Dist_Weighted_P']=weighted['Per_Weight']*weighted['Trip_Distance']\n",
    "\n",
    "print('Person weights and trip weights are all the same: ' ,all(sHPW['Per_Weight']==sHPW['Trip_Weight']))\n",
    "\n",
    "# calculate number of persons using the whole sP file, so we can accuractely calculate km/cap/day. i.e. including those who didn't travel on the survey date\n",
    "unique_persons=sP.loc[:,['HH_PNR','Per_Weight']].drop_duplicates()\n",
    "unique_persons=unique_persons.loc[~unique_persons['HH_PNR'].isin(na_PNR),:]\n",
    "\n",
    "weight_daily_travel=pd.DataFrame(0.001*weighted.groupby('Mode')['Dist_Weighted_P'].sum()/unique_persons['Per_Weight'].sum()).reset_index()\n",
    "commute_avg=0.001*weighted.loc[weighted['Trip_Purpose_Agg']=='Home↔Work','Dist_Weighted_P'].sum()/weighted.loc[weighted['Trip_Purpose_Agg']=='Home↔Work','Per_Weight'].sum()\n",
    "trip_avg=0.001*weighted['Dist_Weighted_P'].sum()/weighted['Per_Weight'].sum()\n",
    "weight_trip_avg=pd.DataFrame(data={'Mode':['All','Commute'],'Avg_trip_dist':[trip_avg,commute_avg]})\n",
    "\n",
    "weight_daily_travel.rename(columns={'Dist_Weighted_P':'Daily_Travel_cap'},inplace=True)\n",
    "weight_daily_travel['Mode_Share']=weight_daily_travel['Daily_Travel_cap']/weight_daily_travel['Daily_Travel_cap'].sum()\n",
    "\n",
    "# calculate car ownership for all households\n",
    "carown=sH.loc[:,['HHNR','HH_Weight','CarOwnershipHH']].drop_duplicates()\n",
    "own=pd.DataFrame(data={'Mode':['Car'],'Ownership':sum(carown['CarOwnershipHH']*carown['HH_Weight'])/sum(carown['HH_Weight'])})\n",
    "weight_daily_travel=weight_daily_travel.merge(own,how='left')\n",
    "weight_daily_travel=weight_daily_travel.merge(weight_trip_avg,how='outer')\n",
    "weight_daily_travel.loc[weight_daily_travel['Mode']=='All','Daily_Travel_cap']=weight_daily_travel['Daily_Travel_cap'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mode</th>\n",
       "      <th>Daily_Travel_cap</th>\n",
       "      <th>Mode_Share</th>\n",
       "      <th>Ownership</th>\n",
       "      <th>Avg_trip_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2_3_Wheel</td>\n",
       "      <td>0.101793</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bike</td>\n",
       "      <td>1.598363</td>\n",
       "      <td>0.106624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Car</td>\n",
       "      <td>8.902532</td>\n",
       "      <td>0.593870</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foot</td>\n",
       "      <td>0.743414</td>\n",
       "      <td>0.049592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transit</td>\n",
       "      <td>3.644605</td>\n",
       "      <td>0.243124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All</td>\n",
       "      <td>14.990707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.016166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Commute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.885428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mode  Daily_Travel_cap  Mode_Share  Ownership  Avg_trip_dist\n",
       "0  2_3_Wheel          0.101793    0.006790        NaN            NaN\n",
       "1       Bike          1.598363    0.106624        NaN            NaN\n",
       "2        Car          8.902532    0.593870   0.707317            NaN\n",
       "3       Foot          0.743414    0.049592        NaN            NaN\n",
       "4    Transit          3.644605    0.243124        NaN            NaN\n",
       "5        All         14.990707         NaN        NaN       5.016166\n",
       "6    Commute               NaN         NaN        NaN       9.885428"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_daily_travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FR(city):\n",
    "    print(city)\n",
    "    if city == 'Clermont':\n",
    "        fn_hh='../../MSCA_data/FranceRQ/lil-0924_Clermont.csv/Csv/Fichiers_Standard_Face_a_face/clermontfer_2012_std_faf_men.csv'\n",
    "        fn_p='../../MSCA_data/FranceRQ/lil-0924_Clermont.csv/Csv/Fichiers_Standard_Face_a_face/clermontfer_2012_std_faf_pers.csv'\n",
    "        fn_t='../../MSCA_data/FranceRQ/lil-0924_Clermont.csv/Csv/Fichiers_Standard_Face_a_face/clermontfer_2012_std_faf_depl.csv'\n",
    "        fn_hh0='../../MSCA_data/FranceRQ/lil-0924_Clermont.csv/Csv/Fichiers_Original_Face_a_face/clermontfer_2012_ori_faf_men.csv'\n",
    "\n",
    "    if city == 'Toulouse':\n",
    "        fn_hh='../../MSCA_data/FranceRQ/lil-0933_Toulouse.csv/Csv/Fichiers_Standard/toulouse_2013_std_men.csv'\n",
    "        fn_p='../../MSCA_data/FranceRQ/lil-0933_Toulouse.csv/Csv/Fichiers_Standard/toulouse_2013_std_pers.csv'\n",
    "        fn_t='../../MSCA_data/FranceRQ/lil-0933_Toulouse.csv/Csv/Fichiers_Standard/toulouse_2013_std_depl.csv'\n",
    "        fn_hh0='../../MSCA_data/FranceRQ/lil-0933_Toulouse.csv/Csv/Fichiers_Original/toulouse_2013_ori_men.csv'\n",
    "\n",
    "    if city == 'Montpellier':\n",
    "        fn_hh='../../MSCA_data/FranceRQ/lil-0937_Montpellier.csv/Csv/Fichiers_Standard_Face_a_face/montpellier_2014_std_faf_men.csv'\n",
    "        fn_p='../../MSCA_data/FranceRQ/lil-0937_Montpellier.csv/Csv/Fichiers_Standard_Face_a_face/montpellier_2014_std_faf_pers.csv'\n",
    "        fn_t='../../MSCA_data/FranceRQ/lil-0937_Montpellier.csv/Csv/Fichiers_Standard_Face_a_face/montpellier_2014_std_faf_depl.csv'\n",
    "\n",
    "    if city == 'Lyon':\n",
    "        fn_hh='../../MSCA_data/FranceRQ/lil-1023_Lyon.csv/Csv/Fichiers_Standard_Face_a_face/lyon_2015_std_faf_men.csv'\n",
    "        fn_p='../../MSCA_data/FranceRQ/lil-1023_Lyon.csv/Csv/Fichiers_Standard_Face_a_face/lyon_2015_std_faf_pers.csv'\n",
    "        fn_t='../../MSCA_data/FranceRQ/lil-1023_Lyon.csv/Csv/Fichiers_Standard_Face_a_face/lyon_2015_std_faf_depl.csv'\n",
    "\n",
    "    if city == 'Nantes':\n",
    "        fn_hh='../../MSCA_data/FranceRQ/lil-1024_Nantes.csv/Csv/Fichiers_Standard_Face_a_face/nantes_2015_std_faf_men.csv'\n",
    "        fn_p='../../MSCA_data/FranceRQ/lil-1024_Nantes.csv/Csv/Fichiers_Standard_Face_a_face/nantes_2015_std_faf_pers.csv'\n",
    "        fn_t='../../MSCA_data/FranceRQ/lil-1024_Nantes.csv/Csv/Fichiers_Standard_Face_a_face/nantes_2015_std_faf_depl.csv'\n",
    "\n",
    "    if city == 'Nimes':\n",
    "        fn_hh='../../MSCA_data/FranceRQ/lil-1135_Nimes.csv/Csv/Fichiers_Standard/nimes_2015_std_men.csv'\n",
    "        fn_p='../../MSCA_data/FranceRQ/lil-1135_Nimes.csv/Csv/Fichiers_Standard/nimes_2015_std_pers.csv'\n",
    "        fn_t='../../MSCA_data/FranceRQ/lil-1135_Nimes.csv/Csv/Fichiers_Standard/nimes_2015_std_depl.csv'\n",
    "\n",
    "    if city == 'Lille':\n",
    "        fn_hh='../../MSCA_data/FranceRQ/lil-1152_Lille.csv/Csv/Fichiers_Standard/lille_2016_std_men.csv'\n",
    "        fn_p='../../MSCA_data/FranceRQ/lil-1152_Lille.csv/Csv/Fichiers_Standard/lille_2016_std_pers.csv'\n",
    "        fn_t='../../MSCA_data/FranceRQ/lil-1152_Lille.csv/Csv/Fichiers_Standard/lille_2016_std_depl.csv'\n",
    "\n",
    "    if city == 'Dijon':\n",
    "        fn_hh='../../MSCA_data/FranceRQ/lil-1214_Dijon.csv/Csv/Fichiers_Standard_Face_a_face/dijon_2016_std_faf_men.csv'\n",
    "        fn_p='../../MSCA_data/FranceRQ/lil-1214_Dijon.csv/Csv/Fichiers_Standard_Face_a_face/dijon_2016_std_faf_pers.csv'\n",
    "        fn_t='../../MSCA_data/FranceRQ/lil-1214_Dijon.csv/Csv/Fichiers_Standard_Face_a_face/dijon_2016_std_faf_depl.csv'\n",
    "\n",
    "    sH=pd.read_csv(fn_hh,sep=';')\n",
    "    sP=pd.read_csv(fn_p,sep=';')\n",
    "    sW=pd.read_csv(fn_t,sep=';')\n",
    "\n",
    "    # load pickled dictionary files\n",
    "    with open('../dictionaries/' + city + '_var.pkl','rb') as f:\n",
    "        var_all = pickle.load(f)\n",
    "\n",
    "    with open('../dictionaries/' + city + '_val.pkl','rb') as f:\n",
    "        value_all = pickle.load(f)\n",
    "\n",
    "    with open('../dictionaries/' + city + '_na.pkl','rb') as f:\n",
    "        na_all = pickle.load(f)\n",
    "\n",
    "    for k in var_all['HH'].keys():\n",
    "        if len(value_all['HH'][k])>0:\n",
    "            sH[k]=sH[var_all['HH'][k]].map(value_all['HH'][k])\n",
    "        elif len(value_all['HH'][k])==0:\n",
    "            sH[k]=sH[var_all['HH'][k]]\n",
    "\n",
    "    for k in var_all['P'].keys():\n",
    "        if len(value_all['P'][k])>0:\n",
    "            sP[k]=sP[var_all['P'][k]].map(value_all['P'][k])\n",
    "        elif len(value_all['P'][k])==0:\n",
    "            sP[k]=sP[var_all['P'][k]]\n",
    "\n",
    "    for k in var_all['W'].keys():\n",
    "        if len(value_all['W'][k])>0:\n",
    "            sW[k]=sW[var_all['W'][k]].map(value_all['W'][k])\n",
    "        elif len(value_all['W'][k])==0:\n",
    "            sW[k]=sW[var_all['W'][k]]\n",
    "\n",
    "    # fill in na's as necessary\n",
    "    sH.fillna(value=na_all['HH'],inplace=True)\n",
    "    sP.fillna(value=na_all['P'],inplace=True)\n",
    "    sW.fillna(value=na_all['W'],inplace=True)\n",
    "\n",
    "    # keep only variables needed, i.e. the variables that are included as dictionary keys\n",
    "    sH=sH[list(value_all['HH'].keys())]\n",
    "    sP=sP[list(value_all['P'].keys())]\n",
    "    sW=sW[list(value_all['W'].keys())]\n",
    "\n",
    "    # define household id !! this is city specific !!\n",
    "    # sH0['HHNR']=sH0['geo_unit'].astype('str')+'_'+sH0['Sample'].astype('str')\n",
    "\n",
    "    # define, sector, zone, and household id !! this is city specific !!\n",
    "\n",
    "    sH['geo_unit'] = sH['Sector_Zone'].apply(lambda y: trunc(0.001*y))\n",
    "    sH['Zone'] = sH['Sector_Zone'].apply(lambda y: y % 1000)\n",
    "    sH['HHNR']=sH['geo_unit'].astype('str')+'_'+sH['Sample'].astype('str')\n",
    "\n",
    "    # define, sector, zone, household, and person id !! this is city specific !!\n",
    "    sP['geo_unit'] = sP['Sector_Zone'].apply(lambda y: trunc(0.001*y))\n",
    "    sP['Zone'] = sP['Sector_Zone'].apply(lambda y: y % 1000)\n",
    "    sP['HHNR']=sP['geo_unit'].astype('str')+'_'+sP['Sample'].astype('str')\n",
    "    sP['HH_PNR']=sP['HHNR']+'_'+sP['Person'].astype('str')\n",
    "\n",
    "    # define, sector, zone, household, and person, and trip id !! this is city specific !!\n",
    "    sW['geo_unit'] = sW['Sector_Zone'].apply(lambda y: trunc(0.001*y))\n",
    "    sW['Zone'] = sW['Sector_Zone'].apply(lambda y: y % 1000)\n",
    "    sW['HHNR']=sW['geo_unit'].astype('str')+'_'+sW['Sample'].astype('str')\n",
    "    sW['HH_PNR']=sW['HHNR']+'_'+sW['Person'].astype('str')\n",
    "    sW['HH_P_WNR']=sW['HH_PNR']+'_'+sW['Trip'].astype('str')\n",
    "\n",
    "    if len(sH['HHNR'].unique())!=len(sH):\n",
    "        print('Unique HHNR not found with sector + sample. Defining HHNR instead with sector_zone + sample.')\n",
    "        sH['HHNR']=sH['Sector_Zone'].astype('str').str.zfill(6)+'_'+sH['Sample'].astype('str')\n",
    "        \n",
    "        sP['HHNR']=sP['Sector_Zone'].astype('str').str.zfill(6)+'_'+sP['Sample'].astype('str')\n",
    "        sP['HH_PNR']=sP['HHNR']+'_'+sP['Person'].astype('str')\n",
    "\n",
    "        sW['HHNR']=sW['Sector_Zone'].astype('str').str.zfill(6)+'_'+sW['Sample'].astype('str')\n",
    "        sW['HH_PNR']=sW['HHNR']+'_'+sW['Person'].astype('str')\n",
    "        sW['HH_P_WNR']=sW['HH_PNR']+'_'+sW['Trip'].astype('str')\n",
    "\n",
    "        if len(sH['HHNR'].unique())!=len(sH):\n",
    "            print('Unique HHNR still not found with sector_zone + sample.')\n",
    "\n",
    "    # merge the household income variable into the household file !! this is only for some cities !!\n",
    "    if 'fn_hh0' in locals():\n",
    "        sH0=pd.read_csv(fn_hh0,sep=';')\n",
    "\n",
    "        for k in var_all['HH0'].keys():\n",
    "            if len(value_all['HH0'][k])>0:\n",
    "                sH0[k]=sH0[var_all['HH0'][k]].map(value_all['HH0'][k])\n",
    "            elif len(value_all['HH0'][k])==0:\n",
    "                sH0[k]=sH0[var_all['HH0'][k]]\n",
    "        sH0=sH0[list(value_all['HH0'].keys())]\n",
    "        sH0['HHNR']=sH0['geo_unit'].astype('str')+'_'+sH0['Sample'].astype('str')\n",
    "        sH0.drop(columns=['Sample','geo_unit'],inplace=True)\n",
    "        sH=sH.merge(sH0,on='HHNR')\n",
    "\n",
    "    # bring HHNR and geo_unit to the left side of the HH df\n",
    "    cols=sH.columns.tolist()\n",
    "    cols_new = ['HHNR', 'geo_unit'] + [value for value in cols if value not in {'HHNR', 'geo_unit'}]\n",
    "    sH=sH[cols_new]\n",
    "\n",
    "    # bring HHNR, HH_PNR, to the left side of the Per df\n",
    "    cols=sP.columns.tolist()\n",
    "    cols_new = ['HHNR','HH_PNR'] + [value for value in cols if value not in {'HHNR', 'HH_PNR'}]\n",
    "    sP=sP[cols_new]\n",
    "\n",
    "    # bring HHNR, HH_PNR, HH_P_WNR to the left side of the W df\n",
    "    cols=sW.columns.tolist()\n",
    "    cols_new = ['HHNR','HH_PNR','HH_P_WNR'] + [value for value in cols if value not in {'HHNR', 'HH_PNR','HH_P_WNR'}]\n",
    "    sW=sW[cols_new]\n",
    "\n",
    "    # calculate household size from the sP data !! This is city specific !! at least in other surevys it is included as it's own variable\n",
    "    hhs=sP['HHNR'].value_counts().reset_index()\n",
    "    hhs.rename(columns={'index':'HHNR','HHNR':'HHSize'},inplace=True)\n",
    "    sH=sH.merge(hhs,on='HHNR')\n",
    "\n",
    "    # address inconsistencies with the Person 'Education' variable, arising from respondents who have completed a certain level of education/training responding no dimploma yet, even though they have lower diplomas than the one they are currently studying for\n",
    "    # these assumptions are based on French law, in which it is mandatory to go to school until age 16 (end of secondary school), so anyone in an occupation post-16 is very likely to have some education. https://www.expatica.com/fr/education/children-education/french-education-system-101147/\n",
    "    # they may need defined differently for non-French surveys\n",
    "    sP.loc[(sP['Age']>11) & (sP['Age']<17) & (sP['Education']=='No diploma yet'),'Education']=\"Elementary\" # if aged between 12 and 16, assume at least an Elementary education\n",
    "    sP.loc[(sP['Age']>16) & (sP['Age']<20) & (sP['Education']=='No diploma yet'),'Education']=\"Secondary\" # if aged between 17 and 19, assume at least a Secondary education.\n",
    "    sP.loc[(sP['Age']>15) & (sP['Occupation']=='Student_3rdLevel') & (sP['Education']=='No diploma yet'),'Education']=\"Secondary+BAC\" # if a 3rd level student, assume at least Secondary eduction with BAC\n",
    "    sP.loc[(sP['Age']>15) & (sP['Occupation']=='Trainee') & (sP['Education']=='No diploma yet'),'Education']=\"Secondary\" # If a trainee, assume at least a secondary education\n",
    "\n",
    "    # address the NA values for Education and Occupation for children under 5\n",
    "    sP.loc[sP['Age']<5,['Education','Occupation']]='Pre-School'\n",
    "\n",
    "    # combine the two 'car parking available' variables \n",
    "    sP['Work/Study_CarParkAvailable']=0\n",
    "    sP.loc[(sP['Work/Study_CarParkAvailable1']==1) | (sP['Work/Study_CarParkAvailable2']==1),'Work/Study_CarParkAvailable']=1\n",
    "    sP.drop(columns=['Sector_Zone','Zone','Sample','geo_unit','Work/Study_CarParkAvailable1','Work/Study_CarParkAvailable2'],inplace=True)\n",
    "\n",
    "    # create the origin and destination id's based on what we extracted from the microdata !! this is city specific !!\n",
    "    sW['Ori_geo_unit']=sW['Ori_Sec_Zone'].apply(lambda y: trunc(0.001*y))\n",
    "    sW['Des_geo_unit']=sW['Des_Sec_Zone'].apply(lambda y: trunc(0.001*y))\n",
    "\n",
    "    # combine the trip reasons for independent and accompanied travellers !! this is city specific !! \n",
    "    # This may not be achievable, as not all cities specify the origin and desination of the accompanied person, and it may be better then to simply classify the trip reason as accompanying/kids\n",
    "    # sW.loc[sW['Ori_Reason1'].isin([61,62,63,64,71,72,73,74]),'Ori_Reason1']=sW['Ori_Reason2']\n",
    "    # sW.loc[sW['Des_Reason1'].isin([61,62,63,64,71,72,73,74]),'Des_Reason1']=sW['Des_Reason2']\n",
    "\n",
    "    # define the simplified origin and destination reasons. !! This is city specific !! \n",
    "    sW['Ori_Reason_Agg']='Other'\n",
    "    sW.loc[sW['Ori_Reason1'].isin([1,2]),'Ori_Reason_Agg']='Home'\n",
    "    sW.loc[sW['Ori_Reason1'].isin([11,12,13,14,81]),'Ori_Reason_Agg']='Work'\n",
    "    # In Toulouse, 'Personal' refers to two categories: \"Being looked after (childminder, crèche...)\" and \"Receiving care (health)\"\n",
    "    sW.loc[sW['Ori_Reason1'].isin([21,41]),'Ori_Reason_Agg']='Personal' # this was prev. classified as 'Care' but was converted to 'Personal' for ease of harmonization with other cities. \n",
    "    sW.loc[sW['Ori_Reason1'].isin([22,23,24,25,26,27,28,29,96,97]),'Ori_Reason_Agg']='School'\n",
    "    sW.loc[sW['Ori_Reason1'].isin([30,31,32,33,34,35,82,98]),'Ori_Reason_Agg']='Shopping'\n",
    "    sW.loc[sW['Ori_Reason1'].isin([51,52,53,54]),'Ori_Reason_Agg']='Leisure'\n",
    "    sW.loc[sW['Ori_Reason1'].isin([61,62,63,64,71,72,73,74]),'Ori_Reason_Agg']='Accompanying/Kids'\n",
    "\n",
    "    sW['Des_Reason_Agg']='Other'\n",
    "    sW.loc[sW['Des_Reason1'].isin([1,2]),'Des_Reason_Agg']='Home'\n",
    "    sW.loc[sW['Des_Reason1'].isin([11,12,13,14,81]),'Des_Reason_Agg']='Work'\n",
    "    sW.loc[sW['Des_Reason1'].isin([21,41]),'Des_Reason_Agg']='Personal' # this was prev. classified as 'Care' but was converted to 'Personal' for ease of harmonization with other cities.\n",
    "    sW.loc[sW['Des_Reason1'].isin([22,23,24,25,26,27,28,29,96,97]),'Des_Reason_Agg']='School'\n",
    "    sW.loc[sW['Des_Reason1'].isin([30,31,32,33,34,35,82,98]),'Des_Reason_Agg']='Shopping'\n",
    "    sW.loc[sW['Des_Reason1'].isin([51,52,53,54]),'Des_Reason_Agg']='Leisure'\n",
    "    sW.loc[sW['Des_Reason1'].isin([61,62,63,64,71,72,73,74]),'Des_Reason_Agg']='Accompanying/Kids'\n",
    "\n",
    "    sW['trip_type_all']=sW['Ori_Reason_Agg']+'-'+sW['Des_Reason_Agg']\n",
    "\n",
    "    # now calculate the detailed o-d trip purposes, this should be harmonized \n",
    "    sW['Trip_Purpose']='Other'\n",
    "    sW.loc[(sW['Ori_Reason_Agg'].isin(['Home','Personal'])) & (sW['Des_Reason_Agg'].isin(['Home','Personal'])),'Trip_Purpose']='Home↔Personal' #\n",
    "    sW.loc[(sW['Ori_Reason_Agg'].isin(['Home','Accompanying/Kids'])) & (sW['Des_Reason_Agg'].isin(['Home','Accompanying/Kids'])),'Trip_Purpose']='Home↔Companion' #\n",
    "    sW.loc[(sW['Ori_Reason_Agg'].isin(['Work','Accompanying/Kids'])) & (sW['Des_Reason_Agg'].isin(['Work','Accompanying/Kids'])),'Trip_Purpose']='Work↔Companion' #\n",
    "    sW.loc[(sW['Ori_Reason_Agg'].isin(['Home','Other'])) & (sW['Des_Reason_Agg'].isin(['Home','Other'])),'Trip_Purpose']='Other↔Home'\n",
    "    sW.loc[sW['trip_type_all']=='Home-Shopping','Trip_Purpose']='Home-Shopping'\n",
    "    sW.loc[sW['trip_type_all']=='Shopping-Home','Trip_Purpose']='Shopping-Home'\n",
    "    sW.loc[sW['trip_type_all']=='Home-School','Trip_Purpose']='Home-School'\n",
    "    sW.loc[sW['trip_type_all']=='School-Home','Trip_Purpose']='School-Home'\n",
    "    sW.loc[sW['trip_type_all']=='Home-Work','Trip_Purpose']='Home-Work'\n",
    "    sW.loc[sW['trip_type_all']=='Work-Home','Trip_Purpose']='Work-Home'\n",
    "    sW.loc[sW['trip_type_all']=='Home-Leisure','Trip_Purpose']='Home-Leisure'\n",
    "    sW.loc[sW['trip_type_all']=='Leisure-Home','Trip_Purpose']='Leisure-Home'\n",
    "    sW.loc[sW['trip_type_all']=='Shopping-Shopping','Trip_Purpose']='Shopping'\n",
    "    sW.loc[sW['trip_type_all']=='Work-Work','Trip_Purpose']='Work'\n",
    "    sW.loc[sW['trip_type_all']=='Work-Leisure','Trip_Purpose']='Work-Leisure'\n",
    "    sW.loc[sW['trip_type_all']=='Work-Work','Trip_Purpose']='Work'\n",
    "    sW.loc[sW['trip_type_all']=='Work-Leisure','Trip_Purpose']='Work-Leisure'\n",
    "    sW.loc[sW['trip_type_all']=='Work-Shopping','Trip_Purpose']='Work-Shopping'\n",
    "    sW.loc[sW['trip_type_all']=='Leisure-Work','Trip_Purpose']='Leisure-Work'\n",
    "    sW.loc[sW['trip_type_all']=='Leisure-Leisure','Trip_Purpose']='Leisure'\n",
    "    sW.loc[sW['trip_type_all']=='Accompanying/Kids-Accompanying/Kids','Trip_Purpose']='Companion'\n",
    "\n",
    "    # make the aggregated trip purpose\n",
    "    sW['Trip_Purpose_Agg']='Other'\n",
    "    sW.loc[sW['Trip_Purpose'].isin(['Home-Work','Work-Home']),'Trip_Purpose_Agg']='Home↔Work'\n",
    "    sW.loc[sW['Trip_Purpose'].isin(['Home-School','School-Home']),'Trip_Purpose_Agg']='Home↔School'\n",
    "    sW.loc[sW['Trip_Purpose'].isin(['Home-Shopping','Shopping-Home']),'Trip_Purpose_Agg']='Home↔Shopping'\n",
    "    sW.loc[sW['Trip_Purpose'].isin(['Home↔Companion']),'Trip_Purpose_Agg']='Home↔Companion'\n",
    "    sW.loc[sW['Trip_Purpose'].isin(['Home-Leisure','Leisure-Home','Home↔Personal']),'Trip_Purpose_Agg']='Home↔Leisure'\n",
    "\n",
    "    sW.drop(columns=['Sector_Zone','Sample','Zone','geo_unit','HHNR','Person', 'Ori_Reason1','Ori_Reason2', 'Des_Reason1', 'Des_Reason2','Hour','Time'],inplace=True,errors='ignore')\n",
    "\n",
    "    sP['City']=city\n",
    "    sH['City']=city\n",
    "    sW['City']=city\n",
    "\n",
    "    return sP, sH, sW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city0='Clermont'\n",
    "df0=pd.read_csv('../outputs/Combined/' + city0 + '_UF.csv',dtype={'Res_geocode':str})\n",
    "df0.drop(columns=['IncomeDetailed','IncomeHarmonised'],errors='ignore',inplace=True)\n",
    "df0['Commute_Trip']=0\n",
    "df0.loc[df0['Trip_Purpose_Agg']=='Home↔Work','Commute_Trip']=1\n",
    "df0['City']=city0\n",
    "df_all=df0.copy()\n",
    "\n",
    "sP0, sH0, sW0=get_FR(city0)\n",
    "sP_all=sP0.copy()\n",
    "sH_all=sH0.copy()\n",
    "sW_all=sW0.copy()\n",
    "cities0=['Dijon','Lille','Lyon','Montpellier','Nantes','Nimes','Toulouse']\n",
    "for city1 in cities0:\n",
    "        print(city1)\n",
    "        df1=pd.read_csv('../outputs/Combined/' + city1 + '_UF.csv',dtype={'Res_geocode':str})\n",
    "        df1.drop(columns=['IncomeDetailed','IncomeHarmonised'],errors='ignore',inplace=True)\n",
    "        df1['Commute_Trip']=0\n",
    "        df1.loc[df1['Trip_Purpose_Agg']=='Home↔Work','Commute_Trip']=1\n",
    "        df1['City']=city0\n",
    "        if len(df1.columns==df_all.columns):\n",
    "                df_all=pd.concat([df_all,df1])\n",
    "                print(city1, 'added.')\n",
    "\n",
    "        sP1, sH1, sW1=get_FR(city1)\n",
    "        if len(sP1.columns)==len(sP_all.columns):# & (sH1.columns==sH_all.columns) & (sW1.columns==sW_all.columns):\n",
    "                sP_all=pd.concat([sP_all,sP1])\n",
    "                sH_all=pd.concat([sH_all,sH1])\n",
    "                sW_all=pd.concat([sW_all,sW1])\n",
    "                print(city1, 'added again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sHPW=df_all.copy()\n",
    "sP=sP_all.copy()\n",
    "sH=sH_all.copy()\n",
    "sW=sW_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No trip weights, person weights used instead.\n"
     ]
    }
   ],
   "source": [
    "# daily distance per person\n",
    "daily_dist=sHPW.groupby('HH_PNR')['Trip_Distance'].sum().to_frame().reset_index()\n",
    "# daily distance per person, including people with out travel distances, for a variety of reasons\n",
    "daily_dist_all=daily_dist.merge(sP['HH_PNR'],how='right')\n",
    "# get the household-person ids for those who did travel, but their travels were recorded as invalid for some reason\n",
    "HH_PNR_na=sW.loc[sW['HH_PNR'].isin(daily_dist_all.loc[daily_dist_all['Trip_Distance'].isna(),'HH_PNR']),'HH_PNR']\n",
    "na_PNR=HH_PNR_na.drop_duplicates().values.tolist()\n",
    "\n",
    "weighted=sHPW.loc[:,('HH_PNR','Per_Weight','Mode','Trip_Distance','Trip_Purpose_Agg')]\n",
    "weighted=weighted.loc[~weighted['HH_PNR'].isin(na_PNR),:]\n",
    "weighted['Dist_Weighted_P']=weighted['Per_Weight']*weighted['Trip_Distance']\n",
    "\n",
    "if 'Trip_Weight' in sW.columns:\n",
    "    print('Person weights and trip weights are all the same: ' ,all(sHPW['Per_Weight']==sHPW['Trip_Weight']))\n",
    "else: \n",
    "    print('No trip weights, person weights used instead.')\n",
    "\n",
    "# calculate number of persons using the whole sP file, so we can accuractely calculate km/cap/day. i.e. including those who didn't travel on the survey date\n",
    "unique_persons=sP.loc[:,['HH_PNR','Per_Weight']].drop_duplicates()\n",
    "unique_persons=unique_persons.loc[~unique_persons['HH_PNR'].isin(na_PNR),:]\n",
    "\n",
    "weight_daily_travel=pd.DataFrame(0.001*weighted.groupby('Mode')['Dist_Weighted_P'].sum()/unique_persons['Per_Weight'].sum()).reset_index()\n",
    "commute_avg=0.001*weighted.loc[weighted['Trip_Purpose_Agg']=='Home↔Work','Dist_Weighted_P'].sum()/weighted.loc[weighted['Trip_Purpose_Agg']=='Home↔Work','Per_Weight'].sum()\n",
    "trip_avg=0.001*weighted['Dist_Weighted_P'].sum()/weighted['Per_Weight'].sum()\n",
    "weight_trip_avg=pd.DataFrame(data={'Mode':['All','Commute'],'Avg_trip_dist':[trip_avg,commute_avg]})\n",
    "\n",
    "weight_daily_travel.rename(columns={'Dist_Weighted_P':'Daily_Travel_cap'},inplace=True)\n",
    "weight_daily_travel['Mode_Share']=weight_daily_travel['Daily_Travel_cap']/weight_daily_travel['Daily_Travel_cap'].sum()\n",
    "\n",
    "# calcylate car ownership for all households\n",
    "carown=sH.loc[:,['HHNR','HH_Weight','CarOwnershipHH']].drop_duplicates()\n",
    "own=pd.DataFrame(data={'Mode':['Car'],'Ownership':sum(carown['CarOwnershipHH']*carown['HH_Weight'])/sum(carown['HH_Weight'])})\n",
    "weight_daily_travel=weight_daily_travel.merge(own,how='left')\n",
    "weight_daily_travel=weight_daily_travel.merge(weight_trip_avg,how='outer')\n",
    "weight_daily_travel.loc[weight_daily_travel['Mode']=='All','Daily_Travel_cap']=weight_daily_travel['Daily_Travel_cap'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mode</th>\n",
       "      <th>Daily_Travel_cap</th>\n",
       "      <th>Mode_Share</th>\n",
       "      <th>Ownership</th>\n",
       "      <th>Avg_trip_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2_3_Wheel</td>\n",
       "      <td>0.183688</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bike</td>\n",
       "      <td>0.209792</td>\n",
       "      <td>0.012246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Car</td>\n",
       "      <td>12.666457</td>\n",
       "      <td>0.739363</td>\n",
       "      <td>0.781252</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foot</td>\n",
       "      <td>0.823116</td>\n",
       "      <td>0.048047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transit</td>\n",
       "      <td>3.248538</td>\n",
       "      <td>0.189623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All</td>\n",
       "      <td>17.131590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.987259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Commute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.281932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Mode  Daily_Travel_cap  Mode_Share  Ownership  Avg_trip_dist\n",
       "0  2_3_Wheel          0.183688    0.010722        NaN            NaN\n",
       "1       Bike          0.209792    0.012246        NaN            NaN\n",
       "2        Car         12.666457    0.739363   0.781252            NaN\n",
       "3       Foot          0.823116    0.048047        NaN            NaN\n",
       "4    Transit          3.248538    0.189623        NaN            NaN\n",
       "5        All         17.131590         NaN        NaN       4.987259\n",
       "6    Commute               NaN         NaN        NaN       9.281932"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_daily_travel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "city_mob_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
